
\documentclass{acm_proc_article-sp}
\usepackage{float}
\begin{document}

\title{Multi-Market Deal Size Prediction}


\numberofauthors{1} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% 1st. author
\alignauthor
Evelin C. F. de Amorim \\
       \affaddr{Departamento de Ci\^{e}ncia da Computa\c{c}\~{a}o}\\
       \affaddr{Universidade Federal de Minas Gerais}\\
       \email{evelinamorim@ufmg.br}
}

\maketitle
\begin{abstract}

Daily-deals sites (DDSs) are web portals that offer discount coupons for services 
or products. Thus, customers are attracted by the captivate discounts. 
DDSs gain profit by the amount of coupons sold and  merchants, who offer  
services and products, gain profit by increasing the amount of clients 
. However, DDSs know 
beforehand the number of coupons sold, can benefit featured offers and 
bussiness strategies.

\emph{Deal size prediction} is the name of the task that predicts the number 
of coupons sold in a given offer. Some research proposed descriptive models 
to deal size, nonetheless few attempts have been made in order to 
predict deal size. Due to its practical importance, we propose a 
novel method to predict deal size. This method is based on an existing method, 
which considers latent markets in given catalog of deals. The existing method 
assigns only one latent market for a deal, and we propose to 
assign the deal to the most likely markets.

We perform experiments in order to detect 
only error in regression phase. The competition model is still in 
development, but we explain the competition model we intend to 
implement
. The results show evidences that 
include all probable markets,  
for a given deal, increase error. Also, we test another strategy that 
consider only markets with at least 0.3 of probability to own a given deal.

%the results balbla
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%\terms{Theory}

%\keywords{ACM proceedings, \LaTeX, text tagging} % NOT required for Proceedings

\section{Introduction}

Daily-deals sites (DDSs) are popular web portals that offer discount coupons 
for services or products. These coupons includes expiration date, therefore 
it is not available for new customers acquire after a short period of time. 
As DDSs coupons are available for limited time, it is usual customers 
miss coupons. Also, customers have decided soon which coupon to buy. 
Considering that the customers own a limited budget, some coupons 
will be disregarded in favor of others coupons.

Coupons that provide similar services or products can bring doubt to 
consumer mind. Thefore, the number of coupons sold of one deal  
can affect the number of coupons sold of another similar deal. From this ideia, 
Lacerda et. al ~\cite{lacerda2014context} proposed a 
strategy, called \emph{Competitive Business Market Prediction} 
(CPMB),
that assigns each deal in the DDS catalog to a latent 
market. CPMB strategy is composed by three main steps:
the first step divides the catalog of deals into markets 
by using Latent Dirichlet Allocation (LDA); the second step perfoms 
regression training in each market; and the third step executes an 
expectation-maximization  procedure in order to consider the 
competition among the deals in the same market.

Byers et. al ~\cite{BMZ12} performed an experiment in order to evaluate 
the impact of online social network in the sales of discount coupons 
of DDSs. Also, Byers builds a linear model to depict the deal size 
according to the deal features. The authors conclude that predicting 
deal size is challenging and future research can explore merchant information 
from reviews web sites.

Although there is practical importance of deal size prediction task, 
there are a minimal amount of research. For these reason, we proposed 
a method based on Lacerda's research in order to achieve better results. 
Nevertheless, our proposal supposes that deals can belong to more than 
one market. The results show that consider all markets probably introduce 
predictions errors in deal size prediction and 
deal only with more likely markets reduce such predictions errors.
%the results balbla

\section{Methodology}

The methodology that we propose is comprised by three steps: 
separation of the deals of the catalog into markets, regression training 
of the deals, and computing the competition between deals in the same 
market. Althought, our steps are very similar to Lacerda's steps, 
the first step is modified in order to assign each offer to the 
most likely markets.

The following subsections explain the methodology in details and 
the evaluation metric employed to evaluate our results.

\subsection{Algorithm}

The algorithm is composed by three phases: Market identification, 
Regression per Market, and Market Competion Weighting. Next, we will 
describe how these three phases are implemented.

\textbf{Market Identification.} Each deal presents textual features 
like: title, description, 
merchant, and so on. These textual features are extracted 
of each deal and tokenized. \texttt{NLTK}\footnote{www.nltk.org} library 
performs the tokenization of each textual feature of a deal. 
Next, stop words are removed and a stemmer are applied to tokens. 
Both stop words and stemmer processes are perfomed by 
\texttt{NLTK} library, which provides a constant list of words 
for stop words and the Porter's algorithm for stemming.
Also common tokens in the catalog is removed, by common tokens 
we consider tokens that appear in at least 25\%  of documents.

After process textual features,
Latent Dirichlet Allocation (LDA) 
method is applied in order to identify the markets. LDA method 
build probabilistic distributions in order to infer latent topics 
in a given collection. The probabilistic distribution are assembled
based on a defined weight assigned to words. The weights can be, 
for instance, term frequency in 
the document. The weigthing scheme in this research is 
\emph{term spread}, which is defined by the Equation 
\ref{eq:termspread}.

\begin{equation}
    TS(t,d) = \sum_{s \in d} i, \text{where i} \left\{ \begin{aligned} 
    1 & & t \in s \\
    0 & & otherwise \\
    \end{aligned} \right.
    \label{eq:termspread}
\end{equation}


The implementation of LDA executed in experiments is from 
{\v R}eh{\r u}{\v r}ek and Petr Sojka ~\cite{rehureklrec}.
%explicar que features textuais podem ter uma deal

\textbf{Regression per Market.} After the processing of textual features, 
each deal is assigned to 
the most probable markets. Next, one regression model is trained 
in historical data for each market. The regression method is trained in the 
following deal features:

\begin{itemize}
    \item City of the deal;
    \item The absolute value of the discount;
    \item Wether the deal sold out;
    \item Number of the days which the deal is available for costumers;
    \item Day of week that the deal starts;
    \item Category in the DDS web site.
\end{itemize}


Each regression model predicts 
the deal size of the test set, when one deal belongs to more than one 
market, then a weighted mean is computed according to Equation \ref{wpred}, 
where $\sigma(d,t)$ is the predicted value for the deal $d$, if the deal 
belongs to market $t$.

\begin{equation}
    pred(d) = \sum_{d \in S} p(S)\times \sigma(d,S)
    \label{wpred}
\end{equation}

Prediction step is applied 
only to deals of the day and we employ and SVR as 
regression method. 
There are as many SVR's classifiers as latent markets.

\textbf{Market Weighting}. After the building of regressor classifiers, 
weights are computed to consider the impact of the market competition in deal size 
prediction. The processes of computation of market weights start with the partition of 
the catalog into $k$ markets, so that $S =\{S_{m_1},S_{m_2},\cdots,S_{m_k}\}$.
Each deal $q \in S$ belongs to one or more markets, and we call $S_q$ the 
subset of markets that owns $q$. In this step we consider the 
prediction as an weigthted mean of predictions. For each market $S^{j}_q \in S_q$ 
we compute the Equation \ref{sigma}.

\begin{equation}
    \sigma^j(q) = \alpha \times \frac{\sum\limits_{\forall d^j \in S^j_q} f(d^j) \times \rho(S^j_q)}{|S^j_q|} + (1 - \alpha) \times f(q)
    \label{sigma}
\end{equation}

The variable $\rho(S^j)$ is unknown and is estimated by the  
expectation-maximization (EM) algorithm. The Equation \ref{rho} calculates $\rho$ 
in each iteration of EM procedure.

\begin{equation}
    \rho(S^j_q) = \frac{\sum_{\forall d^j \in S^j_q} \sigma(d^j)}{\sum_{\forall d \in S} \sigma(d)}
    \label{rho}
\end{equation}

Then, the prediction for a given deal $q$ is depicted by the weighted mean 
of $\sigma^j$ predictions for $q$. The weight is, again, the probability of $q$ 
belongs to the market $S^j_q$.

The loss function for the expectation-maximization is 
\emph{Root Mean Square Error}(RMSE), which is defined by Equation 
\ref{rmse}.

\begin{equation}
    RMSE(y,\sigma) = \sqrt{\frac{1}{n}\sum_{1\geq i}(y_i - sigma(q))^2}
    \label{rmse}
\end{equation}

, where $\sigma(q)$ is the weithed mean of $\sigma^j(q)$ and $n$ is the number 
of predicted deals.

\subsection{Evaluation}

There are many ways to evaluate recommender systems. However, our task 
is not the recommendation itself but a support task for 
recommender systems. Also, the sizes of the deals can vary greatly  
, hence to evaluate our 
prediction, we normalized our dataset using standard deviation of 
whole collection. Otherwise, 
our predictions errors reach high values and we are unable to 
identify the potential of our proposal. 
We adopt the 
\emph{Root Mean Square Error} (RMSE)  as evaluation metric, which is 
defined by Equation \ref{rmse}. 



\section{Experiments and Results}

The implementation of this research produced a different outcome reported  
by the original~\cite{lacerda2014context}. The reasons for this outcome 
may vary: different parameters for griding search in SVR, different dataset, 
differences in the features of dataset, 
details about expectation-maximization from Lacerda's thesis, and so on.
In face os these issues, we performed alternative experiments. The following 
sections describe the dataset employed and the experiments performed.


\subsection{Dataset}

Dataset employed in the experiments was provided by Byers et. al ~\cite{BMZ12}.
There are two DDSs web portal in Byer's collection: Groupon and Living Social. 
As we demand textual features, and Byers collection's has only structural 
features of the deals, we crawled the web pages from mentioned DDSs portals. 
Nonetheless, some deals are unavailable, therefore the number of the deals 
we tested is different from the numbers reported by Byers.


The deals from living social dataset were collected between March 21st and 
July 3rd, 2011. The textual features from these dataset were crawled in 
September 24th, 2014. From the original dataset, we excluded nineteen deals, 
resulting in 2590 deals. The deals from groupon were tested, however due to 
time constraints only one test was performed. From the original groupon 
dataset, we excluded five deals, resulting in 16685 deals.

We depicted the distribution of deal size of 
both datasets in Figure \ref{dealsize}. The distributions of deal 
sizes resemble an exponential distribution: few deals with low deal sizes. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{dealsizels1.png}
    \includegraphics[scale=0.4]{dealsizegroupon1.png}
    \caption{Distribution of Deal Size in the Dataset}
    \label{dealsize}
\end{figure}


\subsection{Experiments}

Due to the time constraints and difficulties in the interpretation of 
 the strategy's description, it was not possible perform the 
experiments as we wish. 

%guidelines
%1
Lacerda employed an SVR classifier for each market and then 
an EM method. Also, Lacerda performed 
a grid search to find the best SVR classifier for each market 
and SVR rbf kernel was employed. However, we considered linear 
kernel for grid search as well. The linear kernel performs  
efficiently in datasets that are linearly separable. The parameters for 
our grid search were $C = \{0.01,0.1,$$1,5,10,50\}$ and 
$\gamma = \{0,0.001,0.0000001\}$.


%2
The EM procedure was not clear in Lacerda's P.h.d. thesis. The definition of 
the EM method ~\cite{hastie2009elements} comprises the likelihood 
of an mixture statiscal distribution. However, the definition 
provided is unclear whether the likelihood is applied in the 
method. Although the description of  the 
marketing competition phase is vague, we attemped to implement 
the procedure described. Nevertheless, we failed in reproducing 
the EM step.

Despite of the problems in reproducing Larceda's research, we 
attempt to evaluate the  RMSE for SVR strategy. The ideia is 
to test the hypothesis that employing best market to 
predict deal size can be improved considering all the possible 
market or considering the markets above a threshold.

Table \ref{rmsetable} shows the average for thirty markets,  
nine executions for each strategy and 90\% of confidence.
The threshold tested was $0.3$, i.e., 
only the markets that has the probility of $0.3$ or above to 
 one given deal is considered in the weighted mean of such deal. 
The difference between Best-Market 
Strategy and All-Market is 
inconclusive, as well as the difference between Best-Market and 
All-Market. However, the difference between All-Market and 
Threshold-Market is significant. The threshold of Threshold-Market 
probably removes unlikely markets, and then it reduces errors
that All-Markets strategy includes when consider unlikely 
markets. 


\begin{table}[H]
    \centering
    \caption{RMSE for all the strategies and 30 markets in Living Social Dataset}
    \label{rmsetable}
    \begin{tabular}{lc} \hline
	Strategy & RMSE \\ \hline \hline
	Best & 1.06\\
        All & 1.12 \\
	Threshold & 0.99 \\ \hline
    \end{tabular}
\end{table}

The Groupon dataset was tested only once, therefore the results in Table 
\ref{rmsetablegroupon} are just for curiosity purposes.

\begin{table}[H]
    \centering
    \caption{RMSE for all the strategies and 50 markets in Groupon Dataset}
    \label{rmsetablegroupon}
    \begin{tabular}{lc} \hline
	Strategy & RMSE \\ \hline \hline
	Best & 0.87\\
        All & 1.07 \\
	Threshold & 1.00 \\ \hline
    \end{tabular}
\end{table}

The results we obtain are slightly better that Lacerda reports, 
however our dataset is different and we also experimented 
linear kernel in our grid search. 

The Figure ~\ref{rmsemarkets} presents a graph of the error 
for different values of markets in Living Social Dataset. The behavior of 
both strategies are similar to the Lacerda's strategy. We observe that 
the behavior of \emph{Best} strategy is consistently better 
than \emph{All} strategy. While the \emph{threshold} strategy is better 
than \emph{All} strategy. The \emph{Best} strategy perform better than 
the other two strategies.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{rrsemarkets.png}
    \caption{RMSE vs. \#Markets for Living Social Dataset}
    \label{rmsemarkets}
\end{figure}


The errors for each topic in one execution  in Living Social dataset
is presented by Table \ref{errortopic}. In some markets, 
All strategy performed well, this usually happens when 
LDA defined a miscellaneous market and also there is a 
high standard deviation in such markets. However, 
in most markets Threshold and Best Strategy are superior.
Also, in all execution always there are markets that own 
more deals than others markets. This is expected, since 
the dataset itself present some numerous categories, and 
although category is different from the topic concept, 
it is an evidence that the dataset is unbalanced.

\begin{table}[h]
\centering
\label{errortopic}
\caption{Error by Topic in one Execution in Living Social Dataset}
\begin{tabular}{lllll}\hline
Topic & \#docs & Best & All    & Threshold     \\ \hline \hline
0     & 95    & 0.57  & 0.65 & 0.54 \\
1      & 64    & 0.89 & 1.11 & 1.10 \\
2      & 27    & 1.08 & 1.24 & 1.17 \\
3      & 35    & 0.48 & 0.56 & 0.39 \\
4      & 35    & 1.25 & 1.43 & 1.56 \\
5      & 61    & 2.01 & 2.08 & 2.24 \\
6      & 24    & 0.78 & 0.88 & 0.73 \\
7      & 32    & 0.43 & 0.56 & 0.55 \\
8      & 95    & 0.43 & 0.50 & 0.41 \\
9      & 13    & 2.07 & 2.17 & 2.19 \\
10     & 56    & 0.43 & 0.47 & 0.37 \\
11     & 67    & 1.16 & 1.27 & 1.27 \\
12     & 21    & 1.01 & 1.24 & 1.26 \\
13     & 36    & 0.91 & 1.05 & 1.12 \\
14     & 59    & 0.84 & 0.86 & 0.80 \\
15     & 260   & 1.01 & 1.26 & 1.11 \\
16     & 93    & 1.13 & 1.27 & 1.30 \\
17     & 32    & 0.64 & 0.89 & 0.77 \\
18     & 17    & 1.93 & 1.99 & 0.55 \\
19     & 22    & 0.77 & 0.73 & 0.54 \\
20     & 59    & 0.42 & 0.53 & 0.43 \\
21     & 25    & 0.53 & 0.65 & 0.64 \\
22     & 24    & 1.16 & 1.33 & 0.65 \\
23     & 23    & 1.19 & 1.08 & 0.89 \\
24     & 130   & 1.64 & 1.61 & 1.63 \\
25     & 38    & 0.86 & 1.00 & 0.53 \\
26     & 48    & 0.50 & 0.63 & 0.38 \\
27     & 25    & 0.55 & 0.70 & 0.68 \\
28     & 37    & 1.44 & 1.41 & 1.59 \\
29     & 31    & 0.68 & 0.74 & 0.71 \\ \hline
\end{tabular}
\end{table}



\section{Conclusion and Discussion}

We proposed a modification in Lacerda's research in order to 
obtain better results. However, our strategy seens simple 
to consider the participation of one deal in different markets. 
It is possible that the errors of all markets 
 introduce more errors in the final result. 
 
 More tests must be perfomed  as well. The groupon dataset was not 
 tested and more tests can produce more insightful clues about 
 more sophisticate techniques, which can consider multi-market deals 
 but avoid to introduce errors like the strategy proposed here. 

 Although the strategy here may be introducing errors in the 
 regression prediction, we may investigate whether is this really 
 the case. The threshold strategy also presented an improvement 
over the initial hypothesis that all likely markets can 
contribute to deal size prediction. Also, despite the fact that the difference between 
Best-Market strategy and Threshold-Market strategy is not 
significant, the Threshold-Market performed consistently well 
over all executions. These results encourage us to formulate 
more elaborate strategies that consider multi-market deals. 

The Table \ref{errortopic} shows that the markets own few 
deals to train in SVR. The errors can be higher due to 
few deals in each SVR has to consider in the training 
step. In this case, the groupon dataset can present different 
results since there are more deals and also more 
features to SVR.


Also, the vague description of the expectation-maximization strategy 
of Lacerda's P.h.d. thesis, become difficult to implement 
the strategy in time. Maybe if we employ the multi-market 
strategy in EM algorithm, the outcome can be improved, even though 
 the multi-market strategy 
performed worse compared to best-market for the regression 
module.

More experiments are being made, like the variation in the 
threshold of the \emph{threshold} strategy and Groupon dataset.
%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
\end{document}
